# 07-1: CUDA Introduction

- Mooreâ€™s Law
    
    : íŠ¸ëœì§€ìŠ¤í„° ê°œìˆ˜ê°€ 2ë…„ë§ˆë‹¤ 2ë°°ë¡œ ëŠ˜ì–´ë‚œë‹¤.
    
- Multicore Architectureì˜ í•„ìš”ì„±
    
    : high clock speed(frequency)ë¥¼ êµ¬í˜„í•˜ê¸° ì–´ë µë‹¤
    
    ì™œ? power consumption, heat generationì´ ë„ˆë¬´ í¬ê¸° ë•Œë¬¸ì—.
    

## Many-core GPUs

- Many coreê°€ ì™œ í•„ìš”í•œê°€?
    
    â‡’ ì‹¤ì‹œê°„ ê³ í™”ì§ˆ 3D ê·¸ë˜í”½ êµ¬í˜„ì„ ìœ„í•˜ì—¬.
    
- parallel, multi-threaded, many-core í”„ë¡œì„¸ì„œê°€ ë°œì „í–ˆë‹¤.
- **************GPGPU**************: General Purpose computing on Graphical Processing Unit
    
    ì „í†µì ìœ¼ë¡œëŠ” CPUì—ì„œ ì²˜ë¦¬ë˜ë˜ general purpose ê³„ì‚°ì„ ìˆ˜í–‰í•˜ê¸° ìœ„í•˜ì—¬ GPUë¥¼ ì‚¬ìš©í•˜ëŠ” ê²ƒ.
    

### Processor: Multicore vs Many-core

ğŸŸ  **Multicore: CPU: 2~8 cores**

- ì¼ë°˜ì ìœ¼ë¡œ general purpose ê³„ì‚°
- sequential í”„ë¡œê·¸ë¨ì˜ ì†ë„ë¥¼ í–¥ìƒì‹œí‚¤ê¸° ìœ„í•´ ë…¸ë ¥í•œë‹¤
- ì—¬ëŸ¬ ì½”ì–´ë“¤ë¡œ ì´ë™í•˜ëŠ” ê³¼ì •ì´ ë³µì¡í•˜ë‹¤: out-of order ë¬¸ì œ, instruction issue, ë¶„ê¸° ì˜ˆì¸¡(branch prediction), pipelining, ëŒ€ìš©ëŸ‰ ìºì‹œâ€¦

ğŸŸ  ******Many-core: GPU: 100~3000 cores******

- parallel applicationì˜ ì‹¤í–‰ ì²˜ë¦¬ëŸ‰(execution throughput)ì— ì§‘ì¤‘
- ë‹¨ìˆœí•˜ë‹¤: in-order, single instruction issue
- ë‹¤ìˆ˜ì˜ ì‘ì€ ì½”ì–´ë“¤
- Ex. Nnvidia GPU

# GPU

ê³ ë„ì˜ parallel applicationì„ ìœ„í•´ ë§Œë“¤ì–´ì§.

- high level language(C/C++) ì‚¬ìš©
- ë†’ì€ GFLOPS(ì»´í“¨í„° ì„±ëŠ¥ ìˆ˜ì¹˜)
- ë¹ ë¥¸ ì²˜ë¦¬ì—ëŠ” ë†’ì€ ëŒ€ì—­í­(bandwidth)ê°€ í•„ìš”í•˜ë‹¤.
- simpler memory model, ì ì€ ì œì•½ â†’ ë†’ì€ ëŒ€ì—­í­ í—ˆìš©
- Memory bandwidth: í”„ë¡œì„¸ì„œë¡œ ë°ì´í„°ë¥¼ ì½ê±°ë‚˜ ë©”ëª¨ë¦¬ì— ì €ì¥í•˜ëŠ” ì†ë„

### GPU is specialized for

- Compute-intensive(ì»´í“¨íŒ… ì§‘ì•½ì )
- Highly **data parallel computation**
    
    ê°™ì€ í”„ë¡œê·¸ë¨ì´ ì—¬ëŸ¬ ë°ì´í„°ì—ì„œ parallelí•˜ê²Œ ì‹¤í–‰ëœë‹¤.
    
    Ex. í–‰ë ¬ ê³„ì‚°
    
- Data caching, flow controlë³´ë‹¤ ë°ì´í„°ë¥¼ ì²˜ë¦¬í•˜ëŠ” íŠ¸ëœì§€ìŠ¤í„° ìˆ˜ê°€ ë§ì•„ì§„ë‹¤.

- Graphics renderingì´ í•„ìš”í•œ ê²ƒì€?
    
    Geometry + Pixel processing
    
- ë§ì€ ì• í”Œë¦¬ì¼€ì´ì…˜ ê°œë°œìë“¤ì´ ì§‘ì•½ì ì¸(intensive) SWë¶€ë¶„ì„ GPUë¡œ ì´ë™í–ˆë‹¤.

### Application

- 3D rendering
    
    pixel, ì •ì (vertex) ì„¸íŠ¸ë“¤ì€ parallel threadì— ë§¤í•‘ëœë‹¤.
    
- ë Œë”ë§ëœ ì´ë¯¸ì§€ì˜ í›„ì²˜ë¦¬, ë¹„ë””ì˜¤ ì¸ì½”ë”© ë° ë””ì½”ë”©, ì´ë¯¸ì§€ ìŠ¤ì¼€ì¼ë§, stereo vision, íŒ¨í„´ ì¸ì‹: ì´ë¯¸ì§€ ë¸”ë¡ê³¼ í”½ì…€ì„ parallel processing threadì— ë§¤í•‘í•  ìˆ˜ ìˆë‹¤.
- ë§ì€ ë‹¤ë¥¸ ì•Œê³ ë¦¬ì¦˜ë“¤ì€ data-parellel processingì— ì˜í•´ ë¹¨ë¼ì§„ë‹¤.
    
    ì¼ë°˜ì ì¸ ì‹ í˜¸ ì²˜ë¦¬/ë¬¼ë¦¬í•™ ì‹œë®¬ë ˆì´ì…˜ë¶€í„° ì»´í“¨í„° ê¸ˆìœµ, ì»´í“¨í„° ìƒë¬¼í•™ê¹Œì§€.
    

# CPU vs GPU

<aside>
ğŸ”¹ CPUì™€ GPUëŠ” ë³¸ì§ˆì ìœ¼ë¡œ ë‹¤ë¥¸ design philosophyë¥¼ ê°€ì§„ë‹¤.

</aside>

### CPU

: sequntial code ì„±ëŠ¥ì— ìµœì í™”ë˜ì–´ ìˆë‹¤.

- ì •êµí•œ control logic
    - single threadì˜ ëª…ë ¹ì–´ë¥¼ parallelí•˜ê²Œ ì‹¤í–‰í•˜ê¸° ìœ„í•´.
    - ì‹¬ì§€ì–´ out-of-orderì˜ ê²½ìš°ì—ë„ ì‹¤í–‰í•˜ê¸° ìœ„í•´.
    - branch prediction
- ëŒ€ìš©ëŸ‰ ìºì‹œ ë©”ëª¨ë¦¬
    
    instructionê³¼ data access latencyë¥¼ ì¤„ì´ê¸° ìœ„í•´
    
- ê°•ë ¥í•œ ALU(ì‚°ìˆ  ë…¼ë¦¬ì¥ì¹˜)
    
    operation latencyë¥¼ ì¤„ì´ê¸° ìœ„í•´.
    
- latencyë¥¼ ìµœì†Œí™”í•œë‹¤. (time to complete a task)

![Untitled](07-1%20CUDA%20Introduction%20658e34426c4543369c326e3afeaa55c6/Untitled.png)

### GPU

: multiple threadì˜ ì‹¤í–‰ ì²˜ë¦¬ëŸ‰ì— ìµœì í™”ë˜ì–´ ìˆë‹¤.

- ì›ë˜ëŠ” 3D video gameì„ ìœ„í•´ ë§Œë“¤ì–´ì§.
    
    (í”„ë ˆì„ ë‹¹ ì—„ì²­ë‚œ ìˆ˜ì˜ ë¶€ë™ ì†Œìˆ˜ì  ê³„ì‚°ì´ í•„ìš”)
    
- control logic, ìºì‹œ ë©”ëª¨ë¦¬ ìµœì†Œí™”
    - chip areaê°€ ëŒ€ë¶€ë¶„ floating-point ê³„ì‚° ì „ìš©ìœ¼ë¡œ ì‚¬ìš©ëœë‹¤.
    - ë©”ëª¨ë¦¬ ì²˜ë¦¬ëŸ‰(memory throughput) í–¥ìƒ
- ì—ë„ˆì§€ íš¨ìœ¨ì ì¸ ALU
- numeric computing engineìœ¼ë¡œ ì„¤ê³„ë˜ì—ˆë‹¤.(ë°ì´í„° ë³‘ë ¬í™”)
- simple taskë¥¼ ì²˜ë¦¬í•˜ê¸° ìœ„í•´ ë””ìì¸ë˜ì—ˆë‹¤.
- throughputì„ ìµœëŒ€í™”í•œë‹¤. (number of tasks in fixed time)

### Winning Applications Use Both CPU and GPU

- CPUê°€ ì„±ëŠ¥ì´ ë” ì¢‹ì€ ë¶€ë¶„ë“¤ë„ ìˆì„ ê²ƒ.
    
    ê·¸ëŸ¬ë‹ˆê¹Œ CPUì™€ GPU ë‘˜ ë‹¤ ì“°ì.
    
- sequential partëŠ” CPU, ìˆ˜ì ìœ¼ë¡œ ë§ì€ ë¶€ë¶„(numerically intensive)ì€ GPU ì‚¬ìš©
- CUDA
    
    í”„ë¡œê·¸ë˜ë¨¸ë“¤ì€ C/C++ programming toolì„ ì‚¬ìš©í•  ìˆ˜ ìˆë‹¤.
    
    ë³µì¡í•œ ê·¸ë˜í”½ ì¸í„°í˜ì´ìŠ¤ë¥¼ ì‚¬ìš©í•˜ì§€ ì•Šì•„ë„ ëœë‹¤.
    

# GPU Architecture

![Untitled](07-1%20CUDA%20Introduction%20658e34426c4543369c326e3afeaa55c6/Untitled%201.png)

- ë‹¤ìˆ˜ì˜ simple coreë¡œ êµ¬ì„±ë˜ì–´ ìˆë‹¤.
- ê³ ë„ë¡œ ìŠ¤ë ˆë“œí™”ëœ SM(Streaming Multiprocessor) ë°°ì—´
- ë‘ ê°œ ì´ìƒì˜ SMì´ building blockì„ í˜•ì„±í•œë‹¤.

### GPU chip design

- GPU coreëŠ” stream processorì´ë‹¤.
- SM: Stream processorë¡œ ì´ë£¨ì–´ì§„ ê·¸ë£¹.
    
    SMëŠ” ê¸°ë³¸ì ìœ¼ë¡œ SIMD í”„ë¡œì„¸ì„œì´ë‹¤.
    
    ![Untitled](07-1%20CUDA%20Introduction%20658e34426c4543369c326e3afeaa55c6/Untitled%202.png)
    

# Why more parallelism?

- ì• í”Œë¦¬ì¼€ì´ì…˜ì€ ê³„ì†í•´ì„œ ì†ë„ í–¥ìƒì„ ìš”êµ¬í•œë‹¤.
- GPUë¥¼ ì˜ êµ¬í˜„í•˜ë©´ sequential executionì— ë¹„í•´ 100ë°° ì´ìƒì˜  ì†ë„ í–¥ìƒì„ ë‹¬ì„±í•  ìˆ˜ ìˆë‹¤.
- SuperComputing ì• í”Œë¦¬ì¼€ì´ì…˜
- í–‰ë ¬ ê³„ì‚°ê³¼ ê°™ì´ ë°ì´í„° ë³‘ë ¬ ê³„ì‚°ì´ í•„ìš”í•œ ëª¨ë“  ì• í”Œë¦¬ì¼€ì´ì…˜

# CUDA (Computer Unified Device Architecture)

: Parallel Computing Framework (NVIDIAì—ì„œ ê°œë°œ)

- GPGPU (General Purpose GPU)
- ë³‘ë ¬í™”ë¥¼ transparentí•˜ê²Œ í™•ì¥í•˜ì—¬ ì½”ì–´ë¥¼ ë§ì´ ì‚¬ìš©í•œë‹¤.

### Compute Capability

- ì»´í“¨íŒ… ì¥ì¹˜ì˜ ì¼ë°˜ ì‚¬ì–‘ ë° íŠ¹ì§•
- major revision numberê³¼ mainor revision numberë¡œ ì •ì˜ëœë‹¤.
    
    Ex. 1.3, 2.1 
    
    - 5: maxwell architecture
    - 3: Kepler architecture
    - 2: Fermi architecture
    - 1: Tesla architecture

### CUDA: main features

![Untitled](07-1%20CUDA%20Introduction%20658e34426c4543369c326e3afeaa55c6/Untitled%203.png)

- Heterogeneous(ì´ì§ˆì ) í”„ë¡œê·¸ë˜ë° ëª¨ë¸
- CPU: host
- GPU: device

### CUDA Device and Threads

- Device
    - CPU(host)ì˜ ë³´ì¡° í”„ë¡œì„¸ì„œ.
    - DRAM(device memory)ì— ì—‘ì„¸ìŠ¤í•  ìˆ˜ ìˆë‹¤.
    - ìŠ¤ë ˆë“œë¥¼ ë³‘ë ¬ë¡œ ì‹¤í–‰í•œë‹¤.
    - ì¼ë°˜ì ìœ¼ë¡œ GPUì´ì§€ë§Œ ë‹¤ë¥¸ ìœ í˜•ì˜ parallel processing deviceì¼ ìˆ˜ë„ ìˆë‹¤.
- Data parallel ë¶€ë¶„ì€ device kernelë¡œ í‘œí˜„ëœë‹¤.
    
    device kernelì€ ë§ì€ ìŠ¤ë ˆë“œ ìœ„ì—ì„œ ì‹¤í–‰ëœë‹¤.
    
- GPU thread v.s. CPU thread
    
    GPU: lightweight (ìƒì„± overhead ë§¤ìš° ì ë‹¤)
    
    full efficiencyë¥¼ ìœ„í•´ì„œëŠ” 1000ê°œ ìŠ¤ë ˆë“œ í•„ìš”í•˜ë‹¤.
    

### Processing Flow

1. Main memoryì˜ ë°ì´í„°ë¥¼ GPU memoryë¡œ ë³µì‚¬
2. CPU: processingì„ ì§€ì‹œí•œë‹¤.
3. GPUê°€ ê°ê°ì˜ ì½”ì–´ë¥¼ ë³‘ë ¬ë¡œ ì‹¤í–‰í•œë‹¤.
4. Main memoryëŠ” GPU memoryì—ì„œ ê²°ê³¼ë¥¼ ë³µì‚¬í•œë‹¤.

### Example 1: CUDA Hello world

```c
%%cu
#include <stdio.h>

__global__ void hello_world(void) {
    printf("Hello World \n");
}

int main(void) {
    hello_world<<<1, 5>>>();
    cudaDeviceSynchronize();
    return 0;
}
```

- ê²°ê³¼
    
    ![Untitled](07-1%20CUDA%20Introduction%20658e34426c4543369c326e3afeaa55c6/Untitled%204.png)
    

### C Language Extensions

- **Function Type Qualifiers: `__global__`**
    - device(GPU)ë¡œ ì‹¤í–‰ë˜ëŠ” ë¶€ë¶„ì´ë‹¤.
    - host(CPU)ì—ì„œ ë¶€ë¥¼ ìˆ˜ ìˆë‹¤.
    - í•¨ìˆ˜ëŠ” ë°˜ë“œì‹œ void typeì„ ë°˜í™˜í•´ì•¼ í•œë‹¤.
    - __global__í•¨ìˆ˜ í˜¸ì¶œì€ í•´ë‹¹ í˜¸ì¶œì— ëŒ€í•œ execution configurationì„ ì§€ì •í•´ì•¼ í•œë‹¤.
    
- **Execution configuration**
    
    <<<Grid ë‹¹ block ê°œìˆ˜, Block ë‹¹ Thread ê°œìˆ˜>>>
    
    ë°©ë²• 1. <<<int, int>>>
    
    xê°’ë§Œ ë°”ë€ë‹¤.
    
    ë°©ë²• 2. <<<dim3, dim3>>>
    
    ```c
    dim3 blocks(65535,65535,1)
    dim3 threads(1024,1,1)
    <<<blocks,threads>>>
    ```
    
- **Built-in Variables**
    
    blockIdx = (blockIdx.x, blockIdx.y, blockIdx.z)
    
    threadIdx = (threadIdx.x, threadIdx.y, threadIdx.z)
    
- **Built-in Vector types**
    
    dim3: ì°¨ì›ì„ í‘œê¸°í•˜ê¸° ìœ„í•œ Integer vector type
    

### Grid, Block, Thread

![Untitled](07-1%20CUDA%20Introduction%20658e34426c4543369c326e3afeaa55c6/Untitled%205.png)

- Grid ë‹¹ ìµœëŒ€ Block size
: 65535 * 65535 * 1
- Block ë‹¹ ìµœëŒ€ Thread ì‚¬ì´ì¦ˆ
: 1024 * 1024 * 64ê°œ
- Block ë‹¹ ìµœëŒ€ Thread ìˆ˜
: 1024ê°œ

### Example 2-1: dim3ë¥¼ ì‚¬ìš©í•˜ì§€ ì•Šì€ blockIdx, threadIdx

```c
%%cu
#include <stdio.h>

__global__ void exec_conf(void) {
    int ix = threadIdx.x + blockIdx.x * blockDim.x;
    printf("gridDim = (%d, %d, %d), blockDim = (%d, %d, %d) \n",
           gridDim.x, gridDim.y, gridDim.z,
           blockDim.x, blockDim.y, blockDim.z);
    
    printf("blockIdx = (%d, %d, %d), threadIdx = (%d, %d, %d), arrayIdx: %d \n",
           blockIdx.x, blockIdx.y, blockIdx.z,
           threadIdx.x, threadIdx.y, threadIdx.z, ix);
}

int main(void) {
    exec_conf<<<2, 3>>>();
    cudaDeviceSynchronize();
    return 0;
}
```

- ê²°ê³¼
    
    ![Untitled](07-1%20CUDA%20Introduction%20658e34426c4543369c326e3afeaa55c6/Untitled%206.png)
    
    ëª¨ë“  ìŠ¤ë ˆë“œëŠ” ê°™ì€ gridDim, blockDimì„ ê°€ì§„ë‹¤.
    
    - gridDim = (gridDim.x, gridDim.y, gridDim.x) = (2, 1, 1)
    - blockDim = (blockDim.x, blockDim.y, blockDim.z) = (3, 1, 1)
    
    ì²« ë²ˆì§¸ blockIdxëŠ” (0, 0, 0), ë‘ ë²ˆì§¸ blockIdxëŠ” (1, 0, 0)
    
    ê° blockì˜ ìŠ¤ë ˆë“œì˜ threadIdxëŠ” ê°ê° (0, 0, 0), (1, 0, 0), (2, 0, 0)
    
    ì¦‰, xê°’ë§Œ ë°”ë€Œì—ˆë‹¤.
    
    ë”°ë¼ì„œ ê° threadì˜ identical indexëŠ” ix = **threadIdx.x + blockIdx.x * blockDim.x**
    
    ![Untitled](07-1%20CUDA%20Introduction%20658e34426c4543369c326e3afeaa55c6/Untitled%207.png)
    

### Example 2-2: dim3 ì‚¬ìš©

```c
%%cu
#include <stdio.h>

__global__ void exec_conf(void) {
    int ix = threadIdx.x + blockIdx.x * blockDim.x;
    int iy = threadIdx.y + blockIdx.y * blockDim.y;

    printf("gridDim = (%d, %d, %d), blockDim = (%d, %d, %d) \n",
           gridDim.x, gridDim.y, gridDim.z,
           blockDim.x, blockDim.y, blockDim.z);
    
    printf("blockIdx = (%d, %d, %d), threadIdx = (%d, %d, %d), arrayIdx: (%d, %d) \n",
           blockIdx.x, blockIdx.y, blockIdx.z,
           threadIdx.x, threadIdx.y, threadIdx.z, ix, iy);
}

int main(void) {
    dim3 blocks(2, 2, 1);
    dim3 threads(2, 2, 2);
    exec_conf<<<blocks, threads>>>();
    cudaDeviceSynchronize();
    return 0;
}
```

- ê²°ê³¼
    
    ```c
    gridDim = (2, 2, 1), blockDim = (2, 2, 2) 
    gridDim = (2, 2, 1), blockDim = (2, 2, 2) 
    gridDim = (2, 2, 1), blockDim = (2, 2, 2) 
    ...
    blockIdx = (0, 1, 0), threadIdx = (0, 0, 0), arrayIdx: (0, 2) 
    blockIdx = (0, 1, 0), threadIdx = (1, 0, 0), arrayIdx: (1, 2) 
    blockIdx = (0, 1, 0), threadIdx = (0, 1, 0), arrayIdx: (0, 3) 
    blockIdx = (0, 1, 0), threadIdx = (1, 1, 0), arrayIdx: (1, 3) 
    blockIdx = (0, 1, 0), threadIdx = (0, 0, 1), arrayIdx: (0, 2) 
    blockIdx = (0, 1, 0), threadIdx = (1, 0, 1), arrayIdx: (1, 2) 
    blockIdx = (0, 1, 0), threadIdx = (0, 1, 1), arrayIdx: (0, 3) 
    blockIdx = (0, 1, 0), threadIdx = (1, 1, 1), arrayIdx: (1, 3) 
    blockIdx = (0, 0, 0), threadIdx = (0, 0, 0), arrayIdx: (0, 0) 
    blockIdx = (0, 0, 0), threadIdx = (1, 0, 0), arrayIdx: (1, 0) 
    blockIdx = (0, 0, 0), threadIdx = (0, 1, 0), arrayIdx: (0, 1) 
    blockIdx = (0, 0, 0), threadIdx = (1, 1, 0), arrayIdx: (1, 1) 
    blockIdx = (0, 0, 0), threadIdx = (0, 0, 1), arrayIdx: (0, 0) 
    blockIdx = (0, 0, 0), threadIdx = (1, 0, 1), arrayIdx: (1, 0) 
    blockIdx = (0, 0, 0), threadIdx = (0, 1, 1), arrayIdx: (0, 1) 
    blockIdx = (0, 0, 0), threadIdx = (1, 1, 1), arrayIdx: (1, 1) 
    blockIdx = (1, 1, 0), threadIdx = (0, 0, 0), arrayIdx: (2, 2) 
    blockIdx = (1, 1, 0), threadIdx = (1, 0, 0), arrayIdx: (3, 2) 
    blockIdx = (1, 1, 0), threadIdx = (0, 1, 0), arrayIdx: (2, 3) 
    blockIdx = (1, 1, 0), threadIdx = (1, 1, 0), arrayIdx: (3, 3) 
    blockIdx = (1, 1, 0), threadIdx = (0, 0, 1), arrayIdx: (2, 2) 
    blockIdx = (1, 1, 0), threadIdx = (1, 0, 1), arrayIdx: (3, 2) 
    blockIdx = (1, 1, 0), threadIdx = (0, 1, 1), arrayIdx: (2, 3) 
    blockIdx = (1, 1, 0), threadIdx = (1, 1, 1), arrayIdx: (3, 3) 
    blockIdx = (1, 0, 0), threadIdx = (0, 0, 0), arrayIdx: (2, 0) 
    blockIdx = (1, 0, 0), threadIdx = (1, 0, 0), arrayIdx: (3, 0) 
    blockIdx = (1, 0, 0), threadIdx = (0, 1, 0), arrayIdx: (2, 1) 
    blockIdx = (1, 0, 0), threadIdx = (1, 1, 0), arrayIdx: (3, 1) 
    blockIdx = (1, 0, 0), threadIdx = (0, 0, 1), arrayIdx: (2, 0) 
    blockIdx = (1, 0, 0), threadIdx = (1, 0, 1), arrayIdx: (3, 0) 
    blockIdx = (1, 0, 0), threadIdx = (0, 1, 1), arrayIdx: (2, 1) 
    blockIdx = (1, 0, 0), threadIdx = (1, 1, 1), arrayIdx: (3, 1)
    ```
    
    ![Untitled](07-1%20CUDA%20Introduction%20658e34426c4543369c326e3afeaa55c6/Untitled%208.png)
    

### Example 3: Vector Sum

```c
%%cu
#include <stdio.h>

const int N = 10;

__global__ void add(int *a, int *b, int *c) {
    int tid = threadIdx.x;
    c[tid] = a[tid] + b[tid];
}

int main(void) {
    int a[N], b[N], c[N];          // Host(CPU)ì—ì„œ ì‚¬ìš©ë˜ëŠ” ë°°ì—´
    int *dev_a, *dev_b, *dev_c;    // Device(GPU)ì—ì„œ ì‚¬ìš©ë˜ëŠ” ë°°ì—´

		// GPU ë©”ëª¨ë¦¬ì— ë°°ì—´ì„ ìœ„í•œ ë©”ëª¨ë¦¬ í• ë‹¹
    cudaMalloc( (void**)&dev_a, N * sizeof(int) );
    cudaMalloc( (void**)&dev_b, N * sizeof(int) );
    cudaMalloc( (void**)&dev_c, N * sizeof(int) );

    for (int i=0; i<N; i++) {
        a[i] = -i;
        b[i] = i*i;
    }

		// CPUì—ì„œ GPUë¡œ ë°°ì—´ ë³µì‚¬
    cudaMemcpy( dev_a, a, N*sizeof(int), cudaMemcpyHostToDevice );
    cudaMemcpy( dev_b, b, N*sizeof(int), cudaMemcpyHostToDevice );

    add<<<1, N>>>(dev_a, dev_b, dev_c);

		// GPUì—ì„œ CPUë¡œ ê²°ê³¼(ë°°ì—´ c)ë¥¼ ë³µì‚¬
    cudaMemcpy(c, dev_c, N*sizeof(int), cudaMemcpyDeviceToHost);

    for (int i=0; i<N; i++) {
        printf("%d + %d = %d \n", a[i], b[i], c[i]);
    }

		// GPU ë©”ëª¨ë¦¬ì— í• ë‹¹ëœ ë°°ì—´ë“¤ì„ í•´ì œ 
    cudaFree (dev_a);
    cudaFree (dev_b);
    cudaFree (dev_c);

    return 0;
}
```

- ê²°ê³¼
    
    ![Untitled](07-1%20CUDA%20Introduction%20658e34426c4543369c326e3afeaa55c6/Untitled%209.png)
    
    - ì™œ êµ³ì´ cpu, gpu ë°°ì—´ì„ ë”°ë¡œ ë§Œë“¤ì–´ì„œ ì‚¬ìš©í•˜ëŠ” ê±´ê°€?
        
        GPUê°€ ë³‘ë ¬ ì²˜ë¦¬ì— íŠ¹í™”ë˜ì–´ ìˆê¸° ë•Œë¬¸ì´ë‹¤.
        
        CUDA í”„ë¡œê·¸ë˜ë°ì—ì„œëŠ” CPUì™€ GPU ê°„ì— ë°ì´í„°ë¥¼ ë³µì‚¬í•˜ê³ , GPUì—ì„œ ì—°ì‚°ì„ ìˆ˜í–‰í•œ í›„ ê²°ê³¼ë¥¼ ë‹¤ì‹œ CPUë¡œ ë³µì‚¬í•´ì™€ì•¼ ëœë‹¤.
        
    - ì™œ gpuì—ì„œëŠ” ë°°ì—´ë¡œ í• ë‹¹í•˜ì§€ ì•Šê³  `cudaMalloc`, `cudaMemcpy`ë¥¼ ì‚¬ìš©í•˜ëŠ”ê°€?
        
        ì¼ë°˜ì ìœ¼ë¡œ ì•ˆì „ì„±ê³¼ íš¨ìœ¨ì„±ì„ ìœ„í•´ `cudaMalloc`, `cudaMemcpy`ë¥¼ ì‚¬ìš©í•œë‹¤.
        
    - ë©”ëª¨ë¦¬ í• ë‹¹ ë¶€ë¶„ì´ ì´í•´ê°€ ì˜ ì•ˆ ëœë‹¤. ì™œ `void**` í˜•íƒœì˜ ì´ì¤‘ í¬ì¸í„°ë¡œ í˜•ë³€í™˜?
        
        ì›ë˜ cudaMalloc í•¨ìˆ˜ì˜ ì²« ë²ˆì§¸ ì¸ìˆ˜ëŠ” `void**` í˜•ì‹ìœ¼ë¡œ ì „ë‹¬ë˜ì–´ì•¼ í•œë‹¤.
        
        ```c
        cudaError_t cudaMalloc(void** devPtr, size_t size);
        ```
        
        ì™œëƒí•˜ë©´ `cudaMalloc` í•¨ìˆ˜ê°€ í˜¸ì¶œë˜ê³  ë‚˜ì„œ, í• ë‹¹ëœ ë””ë°”ì´ìŠ¤ ë©”ëª¨ë¦¬ ì£¼ì†Œë¥¼ í¬ì¸í„°ë¡œ ë°˜í™˜í•˜ê¸° ë•Œë¬¸ì´ë‹¤. 
        
        ì´ë¥¼ ìœ„í•´ `devPtr` ë§¤ê°œë³€ìˆ˜ë¥¼ `void**` í˜•ì‹ìœ¼ë¡œ ì „ë‹¬í•˜ì—¬, í•´ë‹¹ ì£¼ì†Œë¥¼ ìˆ˜ì •í•  ìˆ˜ ìˆê²Œ í•´ì•¼ í•œë‹¤.
        
        ì°¸ê³ ë¡œ, `void*`ëŠ” ì–´ë–¤ typeì˜ ë©”ëª¨ë¦¬ ì£¼ì†Œë¥¼ ê°€ë¦¬í‚¬ ìˆ˜ ìˆëŠ” ì¼ë°˜ì ì¸ í¬ì¸í„° typeì´ë‹¤.
        (int*ëŠ” int íƒ€ì…ì˜ ë©”ëª¨ë¦¬ ì£¼ì†Œë¥¼ ê°€ë¦¬í‚¤ê³ , float*ì€ float íƒ€ì…ì„ ê°€ë¦¬í‚¤ëŠ” ê²ƒì²˜ëŸ¼.)
        
        ì¦‰, `void*`ëŠ” ë©”ëª¨ë¦¬ ì£¼ì†Œì— ëŒ€í•œ ì¼ì¢…ì˜ â€œì¼ë°˜ í¬ì¸í„°â€ì´ë‹¤.
        
        ë”°ë¼ì„œ, `void**` í˜•ì‹ìœ¼ë¡œ ì „ë‹¬í•˜ë©´, cudaMalloc í•¨ìˆ˜ê°€ í• ë‹¹ëœ ë©”ëª¨ë¦¬ ì£¼ì†Œë¥¼ í•´ë‹¹ í¬ì¸í„°ì— ì €ì¥í•  ìˆ˜ ìˆë‹¤. (í¬ì¸í„°ì˜ í¬ì¸í„°ë‹ˆê¹Œ, í¬ì¸í„°ê°’ë¥¼ ìˆ˜ì •í•  ìˆ˜ ìˆë‹¤.)