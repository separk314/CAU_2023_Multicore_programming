# 03: Performance of Parallel Programs

# Flynnâ€™s Taxonomy on Parallel Computer(í”Œë¦° ë¶„ë¥˜)

- Classified with two independent dimension
    - Instruction stream (ëª…ë ¹ì–´ ìŠ¤íŠ¸ë¦¼ì˜ ê°œìˆ˜)
    - Data stream (ì²˜ë¦¬ ê°€ëŠ¥í•œ ë°ì´í„° ìŠ¤íŠ¸ë¦¼)
    
    ![Untitled](03%20Performance%20of%20Parallel%20Programs%20b7bfe451df434c159b351625a6de58e3/Untitled.png)
    
    <aside>
    ğŸ’¡ Parallel computerëŠ” í¬ê²Œ **SIMDì™€ MIMD**ë¡œ ë‚˜ëˆŒ ìˆ˜ ìˆë‹¤.
    
    </aside>
    

### 1ï¸âƒ£ SISD

: Single Instruction, Single Data

- A serial (non-parallel) computer
- ê°€ì¥ ì˜¤ë˜ëœ type

![Untitled](03%20Performance%20of%20Parallel%20Programs%20b7bfe451df434c159b351625a6de58e3/Untitled%201.png)

![Untitled](03%20Performance%20of%20Parallel%20Programs%20b7bfe451df434c159b351625a6de58e3/Untitled%202.png)

### 2ï¸âƒ£ SIMD

: **Single Instruction, Multiple Data**

- All processing units execute the same instruction at any given clock cycle.
    
    ëª¨ë“  ìœ ë‹›ë“¤ì´ ê°™ì€ instructionì„ ì‹¤í–‰í•œë‹¤
    
- Best suited for specialized problems characterized by a high degree of regularity (such as graphics/image processing)
    
    ê³ ë„ì˜ ê·œì¹™ì„±ì„ íŠ¹ì§•ìœ¼ë¡œ í•˜ëŠ” ì „ë¬¸ì ì¸ ë¬¸ì œì— ê°€ì¥ ì í•©í•˜ë‹¤ (ì²˜ë¦¬í•  ë°ì´í„°ê°€ ë°©ëŒ€í•œ ê²½ìš°ì— ì í•©, ë³µì¡í•œ êµ¬ì¡°ë¥¼ ê°€ì§€ëŠ” parallel ë¬¸ì œì—ëŠ” ì í•©X)
    
- Ex. GPU

![Untitled](03%20Performance%20of%20Parallel%20Programs%20b7bfe451df434c159b351625a6de58e3/Untitled%203.png)

![Untitled](03%20Performance%20of%20Parallel%20Programs%20b7bfe451df434c159b351625a6de58e3/Untitled%204.png)

### 3ï¸âƒ£ MISD

: Multiple Instruction, Single Data

- Each processing unit operates on the data independently via separate instruction streams
    
    (ê° í”„ë¡œì„¸ì„œëŠ” ë³„ë„ì˜ ëª…ë ¹ ìŠ¤íŠ¸ë¦¼ì„ í†µí•´ ë…ë¦½ì ìœ¼ë¡œ ë°ì´í„°ì—ì„œ ì‘ë™í•œë‹¤)
    
- Few actual examples of this class of parallel computer have ever existed.
    
    (ì‹¤ì œë¡œ êµ¬í˜„ëœ ì»´í“¨í„° X)
    

![Untitled](03%20Performance%20of%20Parallel%20Programs%20b7bfe451df434c159b351625a6de58e3/Untitled%205.png)

### 4ï¸âƒ£ MIMD

: **Multiple Instruction, Multiple Data**

- Every processor may be executing a different instruction system
- Every processor may be working with a different data stream
- the most common type of parallel computer
- Most modern **super-computers** fall into this category

![Untitled](03%20Performance%20of%20Parallel%20Programs%20b7bfe451df434c159b351625a6de58e3/Untitled%206.png)

# Creating a Parallel Program

![Untitled](03%20Performance%20of%20Parallel%20Programs%20b7bfe451df434c159b351625a6de58e3/Untitled%207.png)

1. **Decomposition**: task ë¶„í•´
2. **Assignment**: ìŠ¤ë ˆë“œì— task í• ë‹¹
    
    (1~2: **Partitioning**)
    
3. **Orchestration**: ë©”ëª¨ë¦¬ ë° task ìŠ¤ì¼€ì¥´ë§
4. **Mapping**: CPU(í”„ë¡œì„¸ì„œ)ì— thread í• ë‹¹ 

## 1ï¸âƒ£ Decomposition

- Break up computation into tasks to be divided among processes
- identify concurrency and decide level at which to exploit

![Untitled](03%20Performance%20of%20Parallel%20Programs%20b7bfe451df434c159b351625a6de58e3/Untitled%208.png)

### Domain Decomposition

- Data is decomposed
- Each parallel task then works on a portion of data

ë°ì´í„°ë¥¼ ë¶„í•´í•´ì„œ, ê°ê°ì˜ parallel taskì´ ë°ì´í„° ìœ„ì—ì„œ ì¼í•œë‹¤.

![Untitled](03%20Performance%20of%20Parallel%20Programs%20b7bfe451df434c159b351625a6de58e3/Untitled%209.png)

1. Load balancing
2. per-workout overhead
- **Block**: processor í•˜ë‚˜ ë‹¹ thread 1ê°œ â†’ **less overhead**
- **Cyclic**: taskê°€ ë¼ìš´ë“œë¡œë¹ˆ ë°©ì‹ìœ¼ë¡œ ê³µí‰í•˜ê²Œ ë¶„ë°°ëœë‹¤ â†’ **************************************good load balancing**************************************

â‡’ ìƒí™©ì— ë”°ë¼ ë§ëŠ” ë°©ì‹ì„ ì„ íƒí•´ì„œ ì‚¬ìš©í•œë‹¤.

### Functional Decomposition

- The focus is on the computation that is to be performed rather than on the data.
- Problem is decomposed according to the work that must be done.
- Each task then performs a portion of the overall work.

![Untitled](03%20Performance%20of%20Parallel%20Programs%20b7bfe451df434c159b351625a6de58e3/Untitled%2010.png)

<aside>
ğŸ’¡ - Domain Decomposition: dataì™€ taskë¥¼ ë¶„í•´.
- Functional Decomposition: ê¸°ëŠ¥ì„ ê¸°ì¤€ìœ¼ë¡œ taskë¥¼ ë¶„í•´. dataì„ ë¶„ë¦¬í•˜ì§€ ì•ŠëŠ”ë‹¤.

</aside>

## 2ï¸âƒ£ Assignment

- ìŠ¤ë ˆë“œì— taskë¥¼ í• ë‹¹í•œë‹¤.
    - Balance workload, reduce communication and management cost
    - Together with decomposition, also called partitioning.
- static ë˜ëŠ” dynamically Assign.
- Assignmentì˜ ëª©í‘œ
    - **Balance workload**
    - **Reduced communication costs**

### Partitioning: Decomposition + Assignment

- Static partitioning: something occurs at **compile time**
- Dynamic partitioning: somthing occurs at **runtime**

## 3ï¸âƒ£ Orchestration

- Structing communication and synchronization
- **Organizing data structures in memory and scheduling tasks** temporally.

- Orchestrationì˜ ëª©í‘œ
    - Reduce cost of communication and synchronization as seen by processors
    - Reserve locality of data reference (including data structure organization)

## 4ï¸âƒ£ Mapping

- **Mapping threads to execution units** (CPU cores)
- Parallel application tries to use the entire machine.
- Usually a job for OS
- Mapping decision
    - Place **related threads** (cooperating threads) **on the same processor**
    - **maximize locality, data sharing**
    - **minimize costs of communication, synchronization**

# Performance of Parallel Programs

- performanceì— ì˜í–¥ì„ ë¼ì¹˜ëŠ” ìš”ì¸ë“¤
    - **Decomposition**: **Coverage** of parallelism in algorithm
    - **Assignment**: **Granularity** of partitioning among processors
    - **Orchestration/Mapping**: **Locality** of computation and communication

# 1ï¸âƒ£ Coverage(Amdahlâ€™s Law)

: Potential program speedup is defined by the fraction of code that can be parallelized.

(parallelized ë  ìˆ˜ ìˆëŠ” ì½”ë“œê°€ ì–¼ë§ˆë‚˜ ìˆëŠ”ê°€.)

![Untitled](03%20Performance%20of%20Parallel%20Programs%20b7bfe451df434c159b351625a6de58e3/Untitled%2011.png)

SpeedUp = old running time / new running time

= 100sec / 60sec

= 1.67ë°°

ì¦‰, parallel versionì´ 1.67ë°° ë” ë¹ ë¥´ë‹¤.

### Amdahlâ€™s Law

- p: fraction of work that can be parallelized
- n: the number of processor

![Untitled](03%20Performance%20of%20Parallel%20Programs%20b7bfe451df434c159b351625a6de58e3/Untitled%2012.png)

<Implications of Amdahlâ€™s Law>

- Speedup tens to `1/(1-p)` as number of processors tends to infinity
    
    ë‹¹ì—°í•˜ì§€, í”„ë¡œì„¸ì„œê°€ ë¬´í•œê°œë©´ parallel taskì— ê±¸ë¦¬ëŠ” ì‹œê°„ì´ 0ì´ˆë‹ˆê¹Œ
    
- Parallel programming is worthwhile when **programs have a lot of work that is parallel in nature.**

<Performance Scalability>

- **********************Scalability**********************: the capability of a system to increase total throughput under an increased load when resources are added.
    
    ![Untitled](03%20Performance%20of%20Parallel%20Programs%20b7bfe451df434c159b351625a6de58e3/Untitled%2013.png)
    
    Hardwareê°€ ì¶”ê°€ë˜ë©´ speedupì´ linearí•˜ê²Œ ì¦ê°€í•œë‹¤.
    

# 2ï¸âƒ£ Granularity

: a qualitative mearsure of the ratio of computation to communication

(**communicationì— í•„ìš”í•œ ì—°ì‚°ì˜ ê°œìˆ˜**)

- **Coarse**: relatively **large amounts of computational work** are done between communication events
- **Fine**: relatively **small amounts of computational work** are done between communication events

- Computation stages are typically separated from periods of communication by synchronization events.

- Granularity (from wikipedia)
    - Granularity: the extent to which a system is broken down into small parts
    - Coarse-grained systems: regards large sub-components
    - Fine-grained systems: regards smaller components of which the larger ones are composed

### Fine vs Coarse Granularity

**Coarse-grain Parallelism**

- í° ë©ì–´ë¦¬
    
    â†’ communication ë¹„ìœ¨ì— ë¹„í•´ high computation.
    
- More opportunity for performance increase due to **low communication overhead**
- Less opportunity for performance enhancement due to **worse load balance**.

![Untitled](03%20Performance%20of%20Parallel%20Programs%20b7bfe451df434c159b351625a6de58e3/Untitled%2014.png)

**Fine-grain Parallelism**

- ì‘ì€ ë©ì–´ë¦¬
    
    â†’ communication ë¹„ìœ¨ì— ë¹„í•´ low computation.
    
- Less opportunity for performance enhancement due to **high communication overhead**
- Better opportunity for performance enhancement due to ****************************better load balance****************************.

![Untitled](03%20Performance%20of%20Parallel%20Programs%20b7bfe451df434c159b351625a6de58e3/Untitled%2015.png)

- ì•Œê³ ë¦¬ì¦˜ê³¼ í•˜ë“œì›¨ì–´ì— ë”°ë¼ íš¨ìœ¨ì ì¸ granularityë¥¼ ì„ íƒí•´ì•¼ í•œë‹¤.
- ë³´í†µ ì˜¤ë²„í—¤ë“œëŠ” **communication, synchronization**ì—ì„œ ë°œìƒí•˜ë¯€ë¡œ, **Coarse-grain granularityê°€ ë” ìœ ë¦¬**í•˜ë‹¤.
- **************Fine-grain parallelism**************ì€ ****************************load imbalance**************************** ì˜¤ë²„í—¤ë“œë¥¼ ì¤„ì¼ ìˆ˜ ìˆë‹¤.

## ğŸ”· Load Balancing

- Distributing approximately **equal amounts of work** among tasks so that **all tasks are kept busy all the time**.
- **Minimization of task idle time**.

Ex. if all tasks are subject to a barrier synchronization point, the slowest task will determine the overall performance.

(ê°€ì¥ ëŠë¦° threadê°€ ëë‚  ë•Œê¹Œì§€ ë‹¤ë¥¸ threadë“¤ì€ idle ìƒíƒœë¥¼ ìœ ì§€
â†’ execution time of slowest taskë¥¼ ì¤„ì—¬ì•¼ í•œë‹¤.)

![Untitled](03%20Performance%20of%20Parallel%20Programs%20b7bfe451df434c159b351625a6de58e3/Untitled%2016.png)

- General Load Balancing Problem
    - ì „ì²´ workëŠ” ìµœëŒ€í•œ ë¹¨ë¦¬ ëë‚˜ì•¼ í•œë‹¤.
    - workersëŠ” ë¹„ì‹¸ë‹ˆê¹Œ í•­ìƒ ë°”ì˜ê²Œ ì¼í•˜ë„ë¡ í•´ì•¼í•œë‹¤.
    - workëŠ” ************************************distributed fairly************************************. ê°ì ê°™ì€ ì–‘ì˜ ì¼ì„ ë¶„ë°°ë°›ì•„ì•¼ í•œë‹¤.
    - precedence constraintsê°€ ìˆìœ¼ë¯€ë¡œ(ë²½ì„ ì™„ì„±í•œ ë‹¤ìŒ ì§€ë¶•ì„ ì˜¬ë¦´ ìˆ˜ ìˆëŠ” ê²ƒì²˜ëŸ¼)
    â†’ clever processing orderë¥¼ ì°¾ì•„ì•¼ í•œë‹¤.

- Load Balancing Problem
    
    : ë¹¨ë¦¬ ëë‚œ í”„ë¡œì„¸ì„œëŠ” ë‹¤ë¥¸ í”„ë¡œì„¸ì„œê°€ ëë‚  ë•Œê¹Œì§€ ê¸°ë‹¤ë ¤ì•¼ í•œë‹¤.
    
    â†’ Leads to **********idle time, lowers utilization**********
    

### Static load balancing == Partitioning

- Programmers make decisions and **assign a fixed amount of work** to each processing core.
- **Runtime overhead â†“**
- Works well for homogeneous multicores (ê°™ì€ ì½”ì–´ì—ì„œ ì˜ ì‘ë™)
    - All cores are same
    - Each core has an equal amount of work
- Not so well for heterogenous multicores
    - Some cores may be faster than others
    - Work distribution is uneven

### Dynamic load balancing

- **When one core finishes** its allocated work, **it takes work** from a work queue or a core with the heaviest workload
- Adapt partitioning at runtime to balance load
- **High runtime overhead**
- ë˜ ë‹¤ë¥¸ ë¬¸ì œì : access to shared data â†’ ë³‘ëª© í˜„ìƒ ë°œìƒ
- Ideal for codes where work is uneven, unpredictable, and in heterogenous multicore.

### Granularity and Performance Tradeoffs

1. Load balancing: ì½”ì–´ ê°„ì— ì‘ì—…ì´ ì–¼ë§ˆë‚˜ ì˜ ë¶„ë°°ë˜ëŠ”ê°€? 
2. Synchronization, Communication: Communication Overhead?

## ğŸ”· Communication

Message passingí•  ë•Œ, í”„ë¡œê·¸ë˜ë¨¸ëŠ” ë‹¤ìŒ ìƒí™©ì— ë”°ë¼ communicationì„ orchestrate í•´ì•¼í•œë‹¤.

- Point to Point
- One to All (Broadcast), All to One (Reduce)
- All to All
- One to Several (Scatter), Several to One (Gather)

### Factors to consider for communication

1. Cost of communications
    - Communicationì€ ì˜¤ë²„í—¤ë“œê°€ ìˆë‹¤.
    - Communications frequently require some type of **synchronization** between tasks, which can result in tasks spending time â€˜************waiting************â€™ **instead of doing work**.

1. Latency vs Bandwidth
    
    â€” **Latency**: Aë¶€í„° Bê¹Œì§€ 0ë°”ì´íŠ¸ë¥¼ ë³´ë‚¼ ë•Œ ê±¸ë¦¬ëŠ” ì‹œê°„
    
    â€” **Bandwidth**: ì‹œê°„ ë‹¹ ì†Œí†µí•  ìˆ˜ ìˆëŠ” ë°ì´í„°ì˜ ì–‘
    
    - ì‘ì€ ë©”ì„¸ì§€ë¥¼ ë§ì´ ë³´ë‚´ê¸° â†’ Latency, communication overheads
    - ì‘ì€ ë©”ì„¸ì§€ë“¤ì„ ë¬¶ì–´ì„œ í•˜ë‚˜ì˜ í° ë©”ì„¸ì§€ë¡œ ë³´ë‚´ëŠ” ê²Œ ë” íš¨ìœ¨ì ì¼ ë•Œë„ ìˆë‹¤.

1. Synchronous vs Asynchronous
    
    â€” **Synchronous**: require some type of â€˜handshakingâ€™ between tasks that share data
    
    â€” Asynchronous: transfer data independently from one another
    

1. Scope of communication
    - Point to Point
    - Collective
    
    (ì•ì—ì„œ ë‚˜ì™”ë˜ Broadcast, Reduce, Gather, Scatter)
    

### MPI: Message Passing Library

- MPI: portable specification
    - Not a language or compiler specification
    - Not a specific implementation or product
    - SPMD model (same program, multiple data)
- Multiple communication models allow precise buffer management
(ë‹¤ì–‘í•œ í†µì‹  ëª¨ë¸ì„ í†µí•´ ì •í™•í•œ ë²„í¼ ê´€ë¦¬ ê°€ëŠ¥)
- Extensive collective operations for scalable global communication
(í™•ì¥ ê°€ëŠ¥í•œ ê¸€ë¡œë²Œ ì»¤ë®¤ë‹ˆì¼€ì´ì…˜ì„ ìœ„í•œ ê´‘ë²”ìœ„í•œ ì§‘ë‹¨ ìš´ì˜)

### â‘  Point-to-Point

![Untitled](03%20Performance%20of%20Parallel%20Programs%20b7bfe451df434c159b351625a6de58e3/Untitled%2017.png)

- Basic method of communication between two processors
    - Originating processor â€œsendsâ€ message to destination processor
    - Destination processor then â€œreceivesâ€ the message
- The message commonly includes
    - Data
    - Length of the message
    - Destination address and possibly a tag

### Synchronous vs Asynchronous Messages

- **Asynchronous**: SenderëŠ” ë©”ì„¸ì§€ê°€ ë³´ë‚´ì§„ ê²ƒë§Œ ì•ˆë‹¤ == **Non-Blocking**

![Untitled](03%20Performance%20of%20Parallel%20Programs%20b7bfe451df434c159b351625a6de58e3/Untitled%2018.png)

- **Synchronous**: ë©”ì„¸ì§€ê°€ ë„ì°©í•˜ë©´ Senderê°€ ì•Œì•„ì°¨ë¦°ë‹¤ == **Blocking**

### Blocking vs Non-Blocking Messages

- **Blocking messages**
    - Sender waits until message is transmitted: buffer is empty
    - Receiver waits until message is received: buffer is full
    - Potential for deadlock
    - ì¥ì : stable, safer
    
    Ex. `read(buf, len, file)`, `write` í•¨ìˆ˜: blocking functionì´ë©´ ì½ê¸° ëë‚˜ê¸° ì „ê¹Œì§€ ê¸°ë‹¤ë¦°ë‹¤. (ë³´í†µ ìš°ë¦¬ëŠ” blocking ì‚¬ìš©)
    
- **Non-Blocking messages**
    - Processing continues even if messages hasnâ€™t been transmitted
    - Avoid idle time and deadlocks
    - ì¥ì : faster
    
    Ex. `fread`, `fwrite`: non-blocking functionì—ì„œëŠ” read í•¨ìˆ˜ í˜¸ì¶œí•˜ìë§ˆì return ë˜ê³  ëë‚œë‹¤.
    

### â‘¡ Broadcast

- One processor sends the same information to many other processors

![Untitled](03%20Performance%20of%20Parallel%20Programs%20b7bfe451df434c159b351625a6de58e3/Untitled%2019.png)

### â‘¢ Reduction

Ex. Every processor starts with a value and needs to know the sum of values stored on all processors

- A reduction combines data from all processors and returns it to a single process
- No processor can finish reduction before each processor has contributed a value.
- **Broadcast & Reduction can reduce programming complexity** and may be more efficient in some programs

### Example: Parallel Numerical Integration

ì ë¶„ êµ¬ê°„ì„ ë‚˜ëˆ ì„œ ê³„ì‚°í•˜ëŠ” ì˜ˆì œ

![Untitled](03%20Performance%20of%20Parallel%20Programs%20b7bfe451df434c159b351625a6de58e3/Untitled%2020.png)

```c
static long num_steps = 10000

void main(int argc, char* argv[]) {
		int i_start, i_end, i, myid, numprocs;
		double pi, mypi, x, step, sum = 0.0;

		MPI_Init(&argc, &argv);
		MPI_Comm_size(MPI_COMM_WORLD, &numprocs);    // í”„ë¡œì„¸ì„œ ê°œìˆ˜
		MPI_Comm_rank(MPI_COMM_WORLD, &myid);

		**MPI_BCAST**(&num_steps, 1, MPI, INIT, 0, MPI_COMM_WORLD);
		
		i_start = my_id * (num_steps/numprocs)
		i_end - i_start + (num_steps/numprocs)
		step = 1.0 / (double) num_steps;

		for (i = i_start; i < i_end; i++) {
			x = (i + 0.5) * step;
			sum = sum + 4.0 / (1.0 + x*x);
		}
		mypi = step * sum;    // ë§ˆì§€ë§‰ì— í•œêº¼ë²ˆì— ë°‘ë³€ì„ ê³±í•œë‹¤

		**MPI_REDUCE**(&mypi, &pi, 1, MPI_DOUBLE, **MPI_SUM**, 0, MPI_COMM_WORLD);

		if (myid == 0)
			printf("Pi = %f\n", pi);
		MPI_Finalize();
	)
}
```

### Synchronization

: Coordination of simultaneous events(threads/processes) in order to obtain correct runtime order and avoid unexpected condition.
(ë™ì‹œ ì´ë²¤íŠ¸ ì¡°ì •)

<Types of synchronization>

- Barrier
    
    Any thread/process must stop at this point(barrier) and cannot proceed until all other threads/processes reach this barrier.
    
    ![Untitled](03%20Performance%20of%20Parallel%20Programs%20b7bfe451df434c159b351625a6de58e3/Untitled%2021.png)
    

- Lock/semaphore
    - **Lock**: the first task acquires the lock. This task can then safely (serially) access the protected data or code.
        
        Other tasks can attempt to acquire the lock but must wait until the task that owns the lock releases it.
        
    - Semaphore: ì„¸ë§ˆí¬ì–´ ê°’ì„ 5ë¡œ ì§€ì •í•˜ë©´ 5ê°œì˜ taskê°€ acquire lock.

# 3ï¸âƒ£ Locality

![Untitled](03%20Performance%20of%20Parallel%20Programs%20b7bfe451df434c159b351625a6de58e3/Untitled%2022.png)

- communication: the slow accesses to â€œremoteâ€ data
- ******************************************************Algorithm should do most work on local data******************************************************
- Need to exploit spatial and temporal locality

### Locality of memory access (shared memory)

Parallel computation is ************************************************************************serialized due to memory contention************************************************************************ and lack of bandwidth.

![Untitled](03%20Performance%20of%20Parallel%20Programs%20b7bfe451df434c159b351625a6de58e3/Untitled%2023.png)

A[0], A[4], A[8], A[12] 4ê°œì˜ ë°ì´í„°ê°€ ê°™ì€ ë©”ëª¨ë¦¬ ê³µê°„ì— ìˆë‹¤ê³  ê°€ì •í•´ë³´ì.

ì´ ê²½ìš° ë™ì‹œì— 4ê°œì˜ threadê°€ memoryì— ì ‘ê·¼

â†’ ë©”ëª¨ë¦¬ êµ¬ì¡° ìƒ í•œë²ˆì— ì ‘ê·¼ ë¶ˆê°€

â†’ serialized

â‡’ ì„±ëŠ¥ ì¦ê°€ë¥¼ ìœ„í•´ì„œëŠ” ë©”ëª¨ë¦¬ Distributionì„ ìˆ˜ì •í•´ì•¼í•¨.

Distribute data to relieve contention and increase effective bandwidth.

![ê¸°ì¡´ì˜ memory interface](03%20Performance%20of%20Parallel%20Programs%20b7bfe451df434c159b351625a6de58e3/Untitled%2024.png)

ê¸°ì¡´ì˜ memory interface

![ìˆ˜ì •í•œ memory interface](03%20Performance%20of%20Parallel%20Programs%20b7bfe451df434c159b351625a6de58e3/Untitled%2025.png)

ìˆ˜ì •í•œ memory interface

### Memory Access Latency in Shared Memory Architectures

- **UMA** (Uniform Memory Access)
    - processor(CPU) - Memory: ì–´ë–¤ dataì— ì ‘ê·¼í•˜ë“  ì ‘ê·¼ ì‹œê°„ ê°™ë‹¤.
    - Centrally located memory
    
    ![Untitled](03%20Performance%20of%20Parallel%20Programs%20b7bfe451df434c159b351625a6de58e3/Untitled%2026.png)
    

- **NUMA** (Non-Uniform Memory Access)
    - Memoryê°€ physically partitioned, í•˜ì§€ë§Œ Busë¡œ ì—°ê²°ë˜ì–´ ìˆì–´ logically connected(accessible by all)
    - Processors have the same address space
    - How to organize data affects performance
    - CC-NUMA (**************************************Cache-Coherent NUMA**************************************)
    
    ![Untitled](03%20Performance%20of%20Parallel%20Programs%20b7bfe451df434c159b351625a6de58e3/Untitled%2027.png)
    

### Cache Coherence(ìºì‹œ ì¼ê´€ì„±)

: the uniformity of shared resouce data that ends up stored in multiple local caches.

![Untitled](03%20Performance%20of%20Parallel%20Programs%20b7bfe451df434c159b351625a6de58e3/Untitled%2028.png)

- Problem: When a processor modifies a shared variable in local cache,
different processors may have different value of the variable.
    - ì—¬ëŸ¬ ìºì‹œì—ì„œ variable ë³µì‚¬ë³¸ì„ ê°€ì§ˆ ìˆ˜ ìˆë‹¤
    - í•œ í”„ë¡œì„¸ì„œê°€ ìˆ˜ì •í•˜ëŠ” ê±´ ë‹¤ë¥¸ í”„ë¡œì„¸ìŠ¤ê°€ ì•Œ ìˆ˜ ì—†ë‹¤
    - ìˆ˜ì •ëœì§€ ëª¨ë¥´ê³  ì˜›ë‚  ê°’ì„ ê³„ì† ì‚¬ìš©í•  ìˆ˜ ìˆë‹¤.
        
        â†’ Visibility ë˜ëŠ” cache coherenceë¥¼ ìœ ì§€í•˜ê¸° ìœ„í•œ ëŒ€ì•ˆì´ í•„ìš”í•˜ë‹¤. 
        

- **Snooping cache coherence**
    
    : ìˆ˜ì •í•  ë•Œë§ˆë‹¤ send all requests for data to all processors.
    
    - ì‘ì€ ì‹œìŠ¤í…œì—ì„œëŠ” ì‚¬ìš© ê°€ëŠ¥
    - expensive

- **Directory-based cache coherence**
    
    : directoryì—ì„œ ê³µìœ ë˜ëŠ” ë³€ìˆ˜ëŠ” kepp track
    
    - ëª¨ë“  í”„ë¡œì„¸ì„œê°€ ì•„ë‹Œ, íŠ¹ì • í”„ë¡œì„¸ì„œì—ê²Œë§Œ request ì œê³µ
    - Send point-to-point requests to processors

### ğŸŒŸ Shared Memory Architecture

: all processors to access all memory as global address space.

- ì¥ì 
    - Global address space provides a **user-friendly programming** perspective to memory. (ì œí•œì´ ì—†ë‹¤)
    - Data sharing between tasks is both **fast and uniform** due to the proximity of memory to CPUs

- ë‹¨ì 
    - Lack of scalability between memory and CPUs
    - Programmer responsibility for ******************************synchronization****************************** (distributed systemì—ì„œëŠ” ì‹ ê²½ ì•ˆ ì¨ë„ ë¨)
    - Expense: it becomes increasingly difficult and expensive to design and produce shared memory machines with ever increasing numbers of processors.
    

### ğŸŒŸ Distributed Memory Architecture

![Untitled](03%20Performance%20of%20Parallel%20Programs%20b7bfe451df434c159b351625a6de58e3/Untitled%2029.png)

- Only private(local) memory
- Independent
- require a communication network to connect inter-processor memory

- ì¥ì 
    - Scalable (í™•ì¥í•˜ê¸° ì¢‹ë‹¤)
    - Cost effective (ì‹¸ë‹¤)
- ë‹¨ì 
    - Programmer responsibility of data communication
    - No global memory access
    - Non-uniform memory access time.

### ğŸŒŸ Hybrid Architecture

![Untitled](03%20Performance%20of%20Parallel%20Programs%20b7bfe451df434c159b351625a6de58e3/Untitled%2030.png)

- Combination of Shared/Distributed architecture
- Scalable
- Increased programmer complexity

### Example of Parallel Program: Ray tracing

ê°ê°ì˜ í”½ì…€ì— ëŒ€í•´ parallel program