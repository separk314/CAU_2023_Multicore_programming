# 02-1: Intro

<aside>
ğŸ’¡ CPU == Processor == Core

</aside>

# Multicore Processor

: A single computing component with two or more independent cores.

- Core(CPU): computing unit that reads/executes program instructions.
- ì—¬ëŸ¬ ê°œì˜ ì½”ì–´ë“¤ì´ ë™ì‹œì— ì—¬ëŸ¬ instructionsë¥¼ ëŒë¦°ë‹¤.
Multiple cores run multiple instructions at the same time.
- Multicore processorì˜ ì„±ëŠ¥ì€ software algorithmsì™€ implementì— ì˜ì¡´í•œë‹¤.

### GPU

: CPUì™€ ë‹¤ë¥´ê²Œ, ë§¤ìš° ë§ì€ ì½”ì–´ë“¤ì„ ê°€ì§„ multicore architectures

- CUDA: Compute Unified Device Architecture
    
    parallel computing platform and programming model created by NVIDIA
    
    **GPGPU**(General Purpose Graphics Processing Unit)
    
- OpenCL

# Parallel Computing

### Parallel computing

: **using multiple processors in parallel** to solve problems more quickly than with a single processor.

<Examples of parallel machines>

- A **cluster computer** that contains multiple PCs **combined together with a high speed network.**
- A ********************************************************shared memory multiprocessor******************************************************** by connecting multiple processors to a single memory system.
- A ****************Chip Multi-Processor**************** (******CMP******) contains multiple processors(called cores) on a single chip.

### Parallel Applications

- Image, video processing (encoding, decoding, filtering)
- 3D graphics (rendering, animation)
- 3D gaming
- Simulation (protein folding, climate modeling)
- Machine learning, deep learning

### Good Parallel Program

Writing good parallel programs should be

- Correct (result)
- Good performance
- Scalability
- Load Balance
- Portability
- Hardware Specific Utilization

### Thread/Process

ë‘˜ ë‹¤ Independent sequence of execution

- Process = ************************************************************************An independent program in execution************************************************************************
    - run in separate memory address space
- Thread
    - run in shared memory space in process
    - One process may have multiple threads
- Multi-threaded Program
    
    : a program running with multiple threads that is executed simultaneously.
    
    ![Untitled](02-1%20Intro%2021b6d4e0f3fb4188b9ab26b39e90bf11/Untitled.png)
    

### 1ï¸âƒ£ Parallelism vs Concurrency

- Parallel Programming
    
    : Using additional computational resources to produce an answer faster.
    
    - **Using multiple processors in parallel** to solve problems more quickly than with a single processor.
    - Ex. summing up all the numbers in an array with multiple n processors

- Concurrent Programming
    
    : Correctly and efficiently controlling access **by multiple threads to shared resources.**
    
    - Ex. Multiple threads access the same hash-table

### 2ï¸âƒ£ Parallel Computing vs Distributed Computing

- Parallel Computing
    - all processors may have access to a shared memory to exchange information between processors.
- Distributed Computing
    
    : a collection of autonomous computer systems that are **physically separated, but are connected by a centralized computer network** that is equipped with distributed system software.
    
    - multiple computers communicate through network.
    - **each processor has its own private memory** (distributed memory)
    - executing sub-tasks on different machines and then merging the results.

<aside>
ğŸ’¡ - Cluster Computing: ì—¬ëŸ¬ ëŒ€ì˜ ì»´í“¨í„°ê°€ í•˜ë‚˜ì˜ ì»´í“¨í„°ì²˜ëŸ¼ ë™ì‘
- Distributed Computing: ìƒëŒ€ì ìœ¼ë¡œ ëŠìŠ¨íˆ ê²°í•©. ì—¬ëŸ¬ í•˜ìœ„ ì„œë¹„ìŠ¤ë¡œ ë¶„í• ë  ìˆ˜ ìˆë‹¤.

</aside>

![Untitled](02-1%20Intro%2021b6d4e0f3fb4188b9ab26b39e90bf11/Untitled%201.png)

<Distributed Computing íŠ¹ì§•>

1. Heterogeneity: ì»´í“¨í„°ë“¤ì´ ë‹¬ë¼ë„ O
2. Transparency: invisible, ë³µì¡ì„±ì„ ìˆ¨ê¸´ë‹¤.
3. Fault tolerance: ì§€ì§„, í™”ì¬ ë“±ì˜ ë¬¸ì œê°€ ë°œìƒí•´ë„ operate properly.
4. Scalability: if we want to add more resources â†’ increase scale
5. Concurrency
6. Openness

1. Resource sharing

### 3ï¸âƒ£ Cluster Computing vs Grid Computing

- Cluster Computing
    
    : a set of **loosely connected** computers that work together so that in many respects they can be **viewed as a single system**
    
    - good price/performance
    - memory not shared
- **Grid Computing**
    
    : federation of computer resources from multiple locations to reach a common goal (a large scale distributed system)
    
    - Grids tend to be **more loosely coupled**, **heterogeneous**, and geographically dispersed.
    - Ex. protein folding
    

# Cloud Computing

: **shares networked computing resources** rather than having local servers or personal devices to handle applications.

- ë°˜ëŒ€ë§: local computing, local server
- Ex. AWS, Google cloud, Microsoft Azure
- â€œCloudâ€ == â€œinternetâ€ == â€œa type of Internet-based computingâ€
- different services(servers, storage, applications) are delivered to an userâ€™s computers and smart phones through the Internet.

- **Why Cloud Computing?**
    
    IT serviceë¥¼ ë§Œë“¤ ë•Œ physical spaceë¥¼ ì§ì ‘ ì‚¬ì§€ ì•Šì•„ë„ ëœë‹¤.
    
    ì¦‰, initial setupì´ í•„ìš”ì—†ë‹¤.